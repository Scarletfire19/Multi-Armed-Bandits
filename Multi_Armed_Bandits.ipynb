{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi Armed Bandits.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnFRdbLfVCD6",
        "outputId": "43558061-7e99-4be6-aca1-59ca376d5e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running multi-armed bandits with nActions = 8, eps = 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Experiment 10/1000] n_steps = 500, reward_avg = 0.578\n",
            "[Experiment 20/1000] n_steps = 500, reward_avg = 0.62\n",
            "[Experiment 30/1000] n_steps = 500, reward_avg = 0.652\n",
            "[Experiment 40/1000] n_steps = 500, reward_avg = 0.468\n",
            "[Experiment 50/1000] n_steps = 500, reward_avg = 0.666\n",
            "[Experiment 60/1000] n_steps = 500, reward_avg = 0.646\n",
            "[Experiment 70/1000] n_steps = 500, reward_avg = 0.458\n",
            "[Experiment 80/1000] n_steps = 500, reward_avg = 0.672\n",
            "[Experiment 90/1000] n_steps = 500, reward_avg = 0.488\n",
            "[Experiment 100/1000] n_steps = 500, reward_avg = 0.486\n",
            "[Experiment 110/1000] n_steps = 500, reward_avg = 0.53\n",
            "[Experiment 120/1000] n_steps = 500, reward_avg = 0.67\n",
            "[Experiment 130/1000] n_steps = 500, reward_avg = 0.63\n",
            "[Experiment 140/1000] n_steps = 500, reward_avg = 0.506\n",
            "[Experiment 150/1000] n_steps = 500, reward_avg = 0.422\n",
            "[Experiment 160/1000] n_steps = 500, reward_avg = 0.528\n",
            "[Experiment 170/1000] n_steps = 500, reward_avg = 0.59\n",
            "[Experiment 180/1000] n_steps = 500, reward_avg = 0.692\n",
            "[Experiment 190/1000] n_steps = 500, reward_avg = 0.65\n",
            "[Experiment 200/1000] n_steps = 500, reward_avg = 0.672\n",
            "[Experiment 210/1000] n_steps = 500, reward_avg = 0.496\n",
            "[Experiment 220/1000] n_steps = 500, reward_avg = 0.592\n",
            "[Experiment 230/1000] n_steps = 500, reward_avg = 0.65\n",
            "[Experiment 240/1000] n_steps = 500, reward_avg = 0.612\n",
            "[Experiment 250/1000] n_steps = 500, reward_avg = 0.458\n",
            "[Experiment 260/1000] n_steps = 500, reward_avg = 0.678\n",
            "[Experiment 270/1000] n_steps = 500, reward_avg = 0.42\n",
            "[Experiment 280/1000] n_steps = 500, reward_avg = 0.632\n",
            "[Experiment 290/1000] n_steps = 500, reward_avg = 0.676\n",
            "[Experiment 300/1000] n_steps = 500, reward_avg = 0.634\n",
            "[Experiment 310/1000] n_steps = 500, reward_avg = 0.634\n",
            "[Experiment 320/1000] n_steps = 500, reward_avg = 0.586\n",
            "[Experiment 330/1000] n_steps = 500, reward_avg = 0.694\n",
            "[Experiment 340/1000] n_steps = 500, reward_avg = 0.5\n",
            "[Experiment 350/1000] n_steps = 500, reward_avg = 0.69\n",
            "[Experiment 360/1000] n_steps = 500, reward_avg = 0.674\n",
            "[Experiment 370/1000] n_steps = 500, reward_avg = 0.456\n",
            "[Experiment 380/1000] n_steps = 500, reward_avg = 0.654\n",
            "[Experiment 390/1000] n_steps = 500, reward_avg = 0.702\n",
            "[Experiment 400/1000] n_steps = 500, reward_avg = 0.506\n",
            "[Experiment 410/1000] n_steps = 500, reward_avg = 0.664\n",
            "[Experiment 420/1000] n_steps = 500, reward_avg = 0.706\n",
            "[Experiment 430/1000] n_steps = 500, reward_avg = 0.622\n",
            "[Experiment 440/1000] n_steps = 500, reward_avg = 0.6\n",
            "[Experiment 450/1000] n_steps = 500, reward_avg = 0.692\n",
            "[Experiment 460/1000] n_steps = 500, reward_avg = 0.636\n",
            "[Experiment 470/1000] n_steps = 500, reward_avg = 0.69\n",
            "[Experiment 480/1000] n_steps = 500, reward_avg = 0.604\n",
            "[Experiment 490/1000] n_steps = 500, reward_avg = 0.462\n",
            "[Experiment 500/1000] n_steps = 500, reward_avg = 0.636\n",
            "[Experiment 510/1000] n_steps = 500, reward_avg = 0.506\n",
            "[Experiment 520/1000] n_steps = 500, reward_avg = 0.534\n",
            "[Experiment 530/1000] n_steps = 500, reward_avg = 0.69\n",
            "[Experiment 540/1000] n_steps = 500, reward_avg = 0.654\n",
            "[Experiment 550/1000] n_steps = 500, reward_avg = 0.654\n",
            "[Experiment 560/1000] n_steps = 500, reward_avg = 0.672\n",
            "[Experiment 570/1000] n_steps = 500, reward_avg = 0.562\n",
            "[Experiment 580/1000] n_steps = 500, reward_avg = 0.648\n",
            "[Experiment 590/1000] n_steps = 500, reward_avg = 0.596\n",
            "[Experiment 600/1000] n_steps = 500, reward_avg = 0.66\n",
            "[Experiment 610/1000] n_steps = 500, reward_avg = 0.534\n",
            "[Experiment 620/1000] n_steps = 500, reward_avg = 0.554\n",
            "[Experiment 630/1000] n_steps = 500, reward_avg = 0.656\n",
            "[Experiment 640/1000] n_steps = 500, reward_avg = 0.688\n",
            "[Experiment 650/1000] n_steps = 500, reward_avg = 0.692\n",
            "[Experiment 660/1000] n_steps = 500, reward_avg = 0.63\n",
            "[Experiment 670/1000] n_steps = 500, reward_avg = 0.56\n",
            "[Experiment 680/1000] n_steps = 500, reward_avg = 0.608\n",
            "[Experiment 690/1000] n_steps = 500, reward_avg = 0.648\n",
            "[Experiment 700/1000] n_steps = 500, reward_avg = 0.502\n",
            "[Experiment 710/1000] n_steps = 500, reward_avg = 0.62\n",
            "[Experiment 720/1000] n_steps = 500, reward_avg = 0.696\n",
            "[Experiment 730/1000] n_steps = 500, reward_avg = 0.702\n",
            "[Experiment 740/1000] n_steps = 500, reward_avg = 0.382\n",
            "[Experiment 750/1000] n_steps = 500, reward_avg = 0.696\n",
            "[Experiment 760/1000] n_steps = 500, reward_avg = 0.702\n",
            "[Experiment 770/1000] n_steps = 500, reward_avg = 0.668\n",
            "[Experiment 780/1000] n_steps = 500, reward_avg = 0.714\n",
            "[Experiment 790/1000] n_steps = 500, reward_avg = 0.664\n",
            "[Experiment 800/1000] n_steps = 500, reward_avg = 0.652\n",
            "[Experiment 810/1000] n_steps = 500, reward_avg = 0.564\n",
            "[Experiment 820/1000] n_steps = 500, reward_avg = 0.682\n",
            "[Experiment 830/1000] n_steps = 500, reward_avg = 0.69\n",
            "[Experiment 840/1000] n_steps = 500, reward_avg = 0.704\n",
            "[Experiment 850/1000] n_steps = 500, reward_avg = 0.482\n",
            "[Experiment 860/1000] n_steps = 500, reward_avg = 0.676\n",
            "[Experiment 870/1000] n_steps = 500, reward_avg = 0.458\n",
            "[Experiment 880/1000] n_steps = 500, reward_avg = 0.504\n",
            "[Experiment 890/1000] n_steps = 500, reward_avg = 0.664\n",
            "[Experiment 900/1000] n_steps = 500, reward_avg = 0.484\n",
            "[Experiment 910/1000] n_steps = 500, reward_avg = 0.622\n",
            "[Experiment 920/1000] n_steps = 500, reward_avg = 0.688\n",
            "[Experiment 930/1000] n_steps = 500, reward_avg = 0.688\n",
            "[Experiment 940/1000] n_steps = 500, reward_avg = 0.462\n",
            "[Experiment 950/1000] n_steps = 500, reward_avg = 0.472\n",
            "[Experiment 960/1000] n_steps = 500, reward_avg = 0.686\n",
            "[Experiment 970/1000] n_steps = 500, reward_avg = 0.56\n",
            "[Experiment 980/1000] n_steps = 500, reward_avg = 0.55\n",
            "[Experiment 990/1000] n_steps = 500, reward_avg = 0.662\n",
            "[Experiment 1000/1000] n_steps = 500, reward_avg = 0.568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)\n",
        "\n",
        "class Environment:\n",
        "\n",
        "    def __init__(self, probs):\n",
        "        self.probs = probs  # success probabilities for each arm\n",
        "\n",
        "    def step(self, action):\n",
        "        # Pull arm and get stochastic reward (1 for success, 0 for failure)\n",
        "        return 1 if (np.random.random()  < self.probs[action]) else 0\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nActions, eps):\n",
        "        self.nActions = nActions\n",
        "        self.eps = eps\n",
        "        self.n = np.zeros(nActions, dtype=np.int) # action counts n(a)\n",
        "        self.Q = np.zeros(nActions, dtype=np.float) # value Q(a)\n",
        "\n",
        "    def update_Q(self, action, reward):\n",
        "        # Update Q action-value given (action, reward)\n",
        "        self.n[action] += 1\n",
        "        self.Q[action] += (1.0/self.n[action]) * (reward - self.Q[action])\n",
        "\n",
        "    def get_action(self):\n",
        "        # Epsilon-greedy policy\n",
        "        if np.random.random() < self.eps: # explore\n",
        "            return np.random.randint(self.nActions)\n",
        "        else: # exploit\n",
        "            return np.random.choice(np.flatnonzero(self.Q == self.Q.max()))\n",
        "\n",
        "# Start multi-armed bandit simulation\n",
        "def experiment(probs, N_episodes):\n",
        "    env = Environment(probs) # initialize arm probabilities\n",
        "    agent = Agent(len(env.probs), eps)  # initialize agent\n",
        "    actions, rewards = [], []\n",
        "    for episode in range(N_episodes):\n",
        "        action = agent.get_action() # sample policy\n",
        "        reward = env.step(action) # take step + get reward\n",
        "        agent.update_Q(action, reward) # update Q\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "    return np.array(actions), np.array(rewards)\n",
        "\n",
        "# Settings\n",
        "probs = [0.50, 0.40, 0.50, 0.10,\n",
        "         0.15, 0.20, 0.5, 0.72] # bandit arm probabilities of success\n",
        "N_experiments = 1000 # number of experiments to perform\n",
        "N_steps = 500 # number of steps (episodes)\n",
        "eps = 0.1 # probability of random exploration (fraction)\n",
        "save_fig = True # save file in same directory\n",
        "output_dir = os.path.join(os.getcwd(), \"output\")\n",
        "\n",
        "# Run multi-armed bandit experiments\n",
        "print(\"Running multi-armed bandits with nActions = {}, eps = {}\".format(len(probs), eps))\n",
        "R = np.zeros((N_steps,))  # reward history sum\n",
        "A = np.zeros((N_steps, len(probs)))  # action history sum\n",
        "for i in range(N_experiments):\n",
        "    actions, rewards = experiment(probs, N_steps)  # perform experiment\n",
        "    if (i + 1) % (N_experiments / 100) == 0:\n",
        "        print(\"[Experiment {}/{}] \".format(i + 1, N_experiments) +\n",
        "              \"n_steps = {}, \".format(N_steps) +\n",
        "              \"reward_avg = {}\".format(np.sum(rewards) / len(rewards)))\n",
        "    R += rewards\n",
        "    for j, a in enumerate(actions):\n",
        "        A[j][a] += 1\n",
        "\n",
        "# Plot reward results\n",
        "R_avg =  R / np.float(N_experiments)\n",
        "plt.plot(R_avg, \".\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.grid()\n",
        "ax = plt.gca()\n",
        "plt.xlim([1, N_steps])\n",
        "if save_fig:\n",
        "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
        "    plt.savefig(os.path.join(output_dir, \"rewards.png\"), bbox_inches=\"tight\")\n",
        "else:\n",
        "    plt.show()\n",
        "plt.close()\n",
        "\n",
        "# Plot action results\n",
        "for i in range(len(probs)):\n",
        "    A_pct = 100 * A[:,i] / N_experiments\n",
        "    steps = list(np.array(range(len(A_pct)))+1)\n",
        "    plt.plot(steps, A_pct, \"-\",\n",
        "             linewidth=4,\n",
        "             label=\"Arm {} ({:.0f}%)\".format(i+1, 100*probs[i]))\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Count Percentage (%)\")\n",
        "leg = plt.legend(loc='upper left', shadow=True)\n",
        "plt.xlim([1, N_steps])\n",
        "plt.ylim([0, 100])\n",
        "for legobj in leg.legendHandles:\n",
        "    legobj.set_linewidth(4.0)\n",
        "if save_fig:\n",
        "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
        "    plt.savefig(os.path.join(output_dir, \"actions.png\"), bbox_inches=\"tight\")\n",
        "else:\n",
        "    plt.show()\n",
        "plt.close()"
      ]
    }
  ]
}